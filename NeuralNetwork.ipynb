{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_in, num_hidden, num_out, learning_rate, activation = 'sigmoid', classifier = True):\n",
    "        self.lr = learning_rate\n",
    "        #num output nodes\n",
    "        self.num_out = num_out\n",
    "        #list that maintains layers\n",
    "        self.layers = []\n",
    "        self.classifier = classifier\n",
    "        np.random.seed(40)\n",
    "        #randomly generate starting weights\n",
    "        hidden = [{\"weights\": [np.random.random()-0.5 for j in range(num_in + 1)], \"activation\": activation} for i in range(num_hidden)]\n",
    "        out = [{\"weights\": [np.random.random()-0.5 for j in range(num_hidden + 1)], \"activation\": \"sigmoid\"} for i in range(num_out)]\n",
    "        self.layers.append(hidden)\n",
    "        self.layers.append(out)\n",
    "    \n",
    "    #Relu activation, accepts inputs and applies relu function to each\n",
    "    def relu_activation(self, inp):\n",
    "        return max(0.0, inp)\n",
    "    \n",
    "    def weighted_sum(self, weights, inputs):\n",
    "        lin_sum = weights[-1] #bias\n",
    "        for i in range(len(inputs)):\n",
    "            lin_sum += inputs[i]*weights[i]\n",
    "        return lin_sum\n",
    "    #Sigmoid activation + transfer. was running into issues with exp overflow when I didn't combine\n",
    "    #these steps into a single method and attempted to separate into sigmoid activation followed by multiplying\n",
    "    #by weights\n",
    "    def sigmoid_activation(self, inp):\n",
    "        return 1.0/(1.0 + np.exp(-inp))\n",
    "    \n",
    "    #applies derivative of sigmoid function to some values\n",
    "    def derivative_sigmoid(self, sig):\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    #applies derivative of relu function to some values\n",
    "    def derivative_relu(self, inp):\n",
    "        return 1.0 if inp > 0 else 0.0\n",
    "    \n",
    "    def softmax(self, idx, weighted_sums):\n",
    "        exp_sum = sum([np.exp(num) for num in weighted_sums])\n",
    "        return np.exp(weighted_sums[idx])/exp_sum\n",
    "    \n",
    "    #forward propogate to make predictions by updating the 'out' field of each neuron\n",
    "    def forward(self, input_vector):\n",
    "        for layer in self.layers:\n",
    "            out = []\n",
    "            if layer != self.layers[len(self.layers) - 1]:\n",
    "                for neuron in layer:\n",
    "                    w_sum = self.weighted_sum(neuron['weights'], input_vector)\n",
    "                    #apply sigmoid activation and weights to make neuron's prediction\n",
    "                    if neuron['activation'] == 'sigmoid':\n",
    "                        neuron['out'] = self.sigmoid_activation(w_sum)\n",
    "                    #apply relu activation and weights to make prediction\n",
    "                    else:\n",
    "                        neuron['out'] = self.relu_activation(w_sum)\n",
    "                    out.append(neuron['out'])\n",
    "            else:\n",
    "                if self.classifier:\n",
    "                    w_sums = [self.weighted_sum(neuron['weights'], input_vector) for neuron in layer]\n",
    "                    for i in range(len(layer)):\n",
    "                        neuron = layer[i]\n",
    "                        neuron['out'] = self.softmax(i, w_sums)\n",
    "                        out.append(neuron['out'])\n",
    "                else:\n",
    "                    for neuron in layer:\n",
    "                        neuron['out'] = self.weighted_sum(neuron['weights'], input_vector)\n",
    "                        out.append(neuron['out'])\n",
    "            #represents the input into the next layer, which is a list of all the current layer's neurons' outputs\n",
    "            input_vector = out\n",
    "        #return the predictions for the last layer\n",
    "        return input_vector\n",
    "    \n",
    "    #back propogate to train the NN on the data\n",
    "    def back_propogate(self, expected):\n",
    "        #move backwards, starting from the last layer to the first\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            #the errors on each neuron\n",
    "            errors = []\n",
    "            if i != len(self.layers) - 1: #bias term\n",
    "                for j in range(len(layer)):\n",
    "                    error_sum = 0\n",
    "                    for neuron in self.layers[i+1]:\n",
    "                        error_sum += neuron['weights'][j] * neuron['delta']\n",
    "                    errors.append(error_sum)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    errors.append(expected[j] - layer[j]['out'])\n",
    "            #use the error times the derivative of the activation function to move towards the weights that minimmize\n",
    "            #the error\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                if neuron['activation'] == 'sigmoid':\n",
    "                    neuron['delta'] = errors[j] * self.derivative_sigmoid(neuron['out'])\n",
    "                else:\n",
    "                    neuron['delta'] = errors[j] * self.derivative_relu(neuron['out'])\n",
    "    \n",
    "    #update the weights of each node\n",
    "    def update_w(self, row):\n",
    "        for i in range(len(self.layers)):\n",
    "            layer_input = row[:-1]\n",
    "            #no error on the first layer (input layer)\n",
    "            if i != 0:\n",
    "                layer_input = [neuron['out'] for neuron in self.layers[i-1]]\n",
    "            #use learning rate times the previously calculated delta value times the input into the layer\n",
    "            #to step towards zero-loss weights in the delta direction with step size tuned by learning rate\n",
    "            for neuron in self.layers[i]:\n",
    "                for j in range(len(layer_input)):\n",
    "                    neuron['weights'][j] += self.lr * neuron['delta'] * layer_input[j]\n",
    "            #bias term\n",
    "            neuron['weights'][-1] += self.lr * neuron['delta']\n",
    "    \n",
    "    #train on data\n",
    "    def train(self, train_data, test_data, max_epochs = 1000):\n",
    "        self.graph(max_epochs)\n",
    "        #update over epochs\n",
    "        train_err_list = []\n",
    "        test_err_list = []\n",
    "        for epoch in range(max_epochs):\n",
    "            train_epoch_error = 0\n",
    "            test_epoch_error = 0\n",
    "            \n",
    "            #training data\n",
    "            for row in train_data:\n",
    "                #make predictions using current model\n",
    "                epoch_out = self.forward(row)\n",
    "                if self.classifier:\n",
    "                    #get error of those predictions\n",
    "                    expected = int(row[-1])\n",
    "                    #ce loss\n",
    "                    train_epoch_error += -np.log(epoch_out[expected])\n",
    "                else:\n",
    "                    train_epoch_error += sum([(expected_out_prob[i] - epoch_out[i])**2 for i in range(len(expected_out_prob))])\n",
    "                \n",
    "                #update the weights based on those calculated errors\n",
    "                expected_out_prob = [0] * len(epoch_out)\n",
    "                expected_out_prob[int(row[-1])] = 1\n",
    "                self.back_propogate(expected_out_prob)\n",
    "                self.update_w(row)   \n",
    "            \n",
    "            train_err_list.append(train_epoch_error/len(train_data))\n",
    "            #plot error of this epoch\n",
    "            #plt.plot(epoch, train_epoch_error/len(train_data), 'ro', label = \"training\")\n",
    "            \n",
    "            #test data\n",
    "            for row in test_data:\n",
    "                #make prediction based on current model\n",
    "                predicted = self.predict(row)\n",
    "                test_epoch_error += (predicted - row[-1]) ** 2\n",
    "                \n",
    "            test_err_list.append(test_epoch_error/len(test_data))\n",
    "            #plot performance on test data\n",
    "            #plt.plot(epoch, test_epoch_error/len(test_data), 'bo', label = \"test\")\n",
    "        plt.plot(range(max_epochs), train_err_list,label = \"training\")\n",
    "        plt.plot(range(max_epochs), test_err_list,label = \"testing\")\n",
    "        plt.legend()\n",
    "    #predict label of given row of data\n",
    "    def predict(self, row):\n",
    "        out_probs = self.forward(row)\n",
    "        max_prob = float('-inf')\n",
    "        max_ind = -1\n",
    "        for i in range(len(out_probs)):\n",
    "            if out_probs[i] > max_prob:\n",
    "                max_ind = i\n",
    "                max_prob = out_probs[i]\n",
    "        return max_ind\n",
    "    \n",
    "    #set up the graph\n",
    "    def graph(self, max_epochs):\n",
    "        plt.xlim(0, max_epochs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"mean squared error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.genfromtxt('iris.data', delimiter=',', dtype = None, encoding = None)\n",
    "num_label_data = []\n",
    "\n",
    "iris_dict = {\n",
    "    \"Iris-setosa\": 0,\n",
    "    \"Iris-versicolor\": 1,\n",
    "    \"Iris-virginica\": 2\n",
    "}\n",
    "\n",
    "for row in raw_data:\n",
    "    sep_len, sep_width, pet_len, pet_width, label = row\n",
    "    label = iris_dict[label]\n",
    "    #using the max of each attribute (provided in iris.names on the dataset) to normalize\n",
    "    sep_len /= 7.6\n",
    "    sep_width /= 4.4\n",
    "    pet_len /= 6.9\n",
    "    pet_width /= 2.5\n",
    "    num_label_data.append((sep_len, sep_width, pet_len, pet_width, int(label)))\n",
    "\n",
    "np.random.shuffle(num_label_data)\n",
    "\n",
    "train_test_spl = int(len(raw_data) * 0.85)\n",
    "train = np.array(num_label_data[:train_test_spl])\n",
    "test = np.array(num_label_data[train_test_spl:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork(4, 20, 3, 0.2, activation = 'sigmoid')\n",
    "net.train(train, test, max_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
