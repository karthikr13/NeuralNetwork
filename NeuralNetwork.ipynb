{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_in, num_hidden, num_out, learning_rate, activation = 'sigmoid'):\n",
    "        self.lr = learning_rate\n",
    "        #num output nodes\n",
    "        self.num_out = num_out\n",
    "        #list that maintains layers\n",
    "        self.layers = []\n",
    "        np.random.seed(40)\n",
    "        #randomly generate starting weights\n",
    "        hidden = [{\"weights\": [np.random.random()-0.5 for j in range(num_in + 1)], \"activation\": \"sigmoid\"} for i in range(num_hidden)]\n",
    "        out = [{\"weights\": [np.random.random()-0.5 for j in range(num_hidden + 1)], \"activation\": activation} for i in range(num_out)]\n",
    "        self.layers.append(hidden)\n",
    "        self.layers.append(out)\n",
    "    \n",
    "    #Relu activation, accepts inputs and applies relu function to each\n",
    "    def relu_activation(self, inp):\n",
    "        return max(0.0, inp)\n",
    "    \n",
    "    def weighted_sum(self, weights, inputs):\n",
    "        lin_sum = weights[-1] #bias\n",
    "        for i in range(len(inputs)):\n",
    "            lin_sum += inputs[i]*weights[i]\n",
    "        return lin_sum\n",
    "    #Sigmoid activation + transfer. was running into issues with exp overflow when I didn't combine\n",
    "    #these steps into a single method and attempted to separate into sigmoid activation followed by multiplying\n",
    "    #by weights\n",
    "    def sigmoid_activation(self, inp):\n",
    "        return 1.0/(1.0 + np.exp(-inp))\n",
    "    \n",
    "    #applies derivative of sigmoid function to some values\n",
    "    def derivative_sigmoid(self, sig):\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    #applies derivative of relu function to some values\n",
    "    def derivative_relu(self, inp):\n",
    "        return 1.0 if inp > 0 else 0.0\n",
    "    \n",
    "    #forward propogate to make predictions by updating the 'out' field of each neuron\n",
    "    def forward(self, input_vector):\n",
    "        for layer in self.layers:\n",
    "            out = []\n",
    "            for neuron in layer:\n",
    "                w_sum = self.weighted_sum(neuron['weights'], input_vector)\n",
    "                #apply sigmoid activation and weights to make neuron's prediction\n",
    "                if neuron['activation'] == 'sigmoid':\n",
    "                    neuron['out'] = self.sigmoid_activation(w_sum)\n",
    "                #apply relu activation and weights to make prediction\n",
    "                else:\n",
    "                    neuron['out'] = self.relu_activation(w_sum)\n",
    "                out.append(neuron['out'])\n",
    "            #represents the input into the next layer, which is a list of all the current layer's neurons' outputs\n",
    "            input_vector = out\n",
    "        #return the predictions for the last layer\n",
    "        return input_vector\n",
    "    \n",
    "    #back propogate to train the NN on the data\n",
    "    def back_propogate(self, expected):\n",
    "        #move backwards, starting from the last layer to the first\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            #the errors on each neuron\n",
    "            errors = []\n",
    "            if i != len(self.layers) - 1: #bias term\n",
    "                for j in range(len(layer)):\n",
    "                    error_sum = 0\n",
    "                    for neuron in self.layers[i+1]:\n",
    "                        error_sum += neuron['weights'][j] * neuron['delta']\n",
    "                    errors.append(error_sum)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    errors.append(expected[j] - layer[j]['out'])\n",
    "            #use the error times the derivative of the activation function to move towards the weights that minimmize\n",
    "            #the error\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                if neuron['activation'] == 'sigmoid':\n",
    "                    neuron['delta'] = errors[j] * self.derivative_sigmoid(neuron['out'])\n",
    "                else:\n",
    "                    neuron['delta'] = errors[j] * self.derivative_relu(neuron['out'])\n",
    "    \n",
    "    #update the weights of each node\n",
    "    def update_w(self, row):\n",
    "        for i in range(len(self.layers)):\n",
    "            layer_input = row[:-1]\n",
    "            #no error on the first layer (input layer)\n",
    "            if i != 0:\n",
    "                layer_input = [neuron['out'] for neuron in self.layers[i-1]]\n",
    "            #use learning rate times the previously calculated delta value times the input into the layer\n",
    "            #to step towards zero-loss weights in the delta direction with step size tuned by learning rate\n",
    "            for neuron in self.layers[i]:\n",
    "                for j in range(len(layer_input)):\n",
    "                    neuron['weights'][j] += self.lr * neuron['delta'] * layer_input[j]\n",
    "            #bias term\n",
    "            neuron['weights'][-1] += self.lr * neuron['delta']\n",
    "    \n",
    "    #train on data\n",
    "    def train(self, train_data, test_data, max_epochs = 1000):\n",
    "        self.graph(max_epochs)\n",
    "        #update over epochs\n",
    "        for epoch in range(max_epochs):\n",
    "            train_epoch_error = 0\n",
    "            test_epoch_error = 0\n",
    "            \n",
    "            #training data\n",
    "            for row in train_data:\n",
    "                #make predictions using current model\n",
    "                epoch_out = self.forward(row)\n",
    "                \n",
    "                #get error of those predictions\n",
    "                expected_out_prob = [0 for i in range(self.num_out)]\n",
    "                expected_out_prob[int(row[-1])] = 1\n",
    "                train_epoch_error += sum([(expected_out_prob[i] - epoch_out[i])**2 for i in range(len(expected_out_prob))])\n",
    "                \n",
    "                #update the weights based on those calculated errors\n",
    "                self.back_propogate(expected_out_prob)\n",
    "                self.update_w(row)   \n",
    "            \n",
    "            #plot error of this epoch\n",
    "            plt.plot(epoch, train_epoch_error/len(train_data), 'ro', label = \"training\")\n",
    "            \n",
    "            #test data\n",
    "            for row in test_data:\n",
    "                #make prediction based on current model\n",
    "                predicted = self.predict(row)\n",
    "                test_epoch_error += (predicted - row[-1]) ** 2\n",
    "                \n",
    "            #plot performance on test data\n",
    "            plt.plot(epoch, test_epoch_error/len(test_data), 'bo', label = \"test\")\n",
    "    \n",
    "    #predict label of given row of data\n",
    "    def predict(self, row):\n",
    "        out_probs = self.forward(row)\n",
    "        max_prob = float('-inf')\n",
    "        max_ind = -1\n",
    "        for i in range(len(out_probs)):\n",
    "            if out_probs[i] > max_prob:\n",
    "                max_ind = i\n",
    "                max_prob = out_probs[i]\n",
    "        return max_ind\n",
    "    \n",
    "    #set up the graph\n",
    "    def graph(self, max_epochs):\n",
    "        plt.xlim(0, max_epochs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"mean squared error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.genfromtxt('iris.data', delimiter=',', dtype = None, encoding = None)\n",
    "num_label_data = []\n",
    "\n",
    "iris_dict = {\n",
    "    \"Iris-setosa\": 0,\n",
    "    \"Iris-versicolor\": 1,\n",
    "    \"Iris-virginica\": 2\n",
    "}\n",
    "\n",
    "for row in raw_data:\n",
    "    sep_len, sep_width, pet_len, pet_width, label = row\n",
    "    label = iris_dict[label]\n",
    "    #using the max of each attribute (provided in iris.names on the dataset) to normalize\n",
    "    sep_len /= 7.6\n",
    "    sep_width /= 4.4\n",
    "    pet_len /= 6.9\n",
    "    pet_width /= 2.5\n",
    "    num_label_data.append((sep_len, sep_width, pet_len, pet_width, int(label)))\n",
    "\n",
    "np.random.shuffle(num_label_data)\n",
    "\n",
    "train_test_spl = int(len(raw_data) * 0.85)\n",
    "train = np.array(num_label_data[:train_test_spl])\n",
    "test = np.array(num_label_data[train_test_spl:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = NeuralNetwork(4, 20, 3, 0.2, activation = 'linear')\n",
    "net.train(train, test, max_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
