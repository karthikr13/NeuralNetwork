{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_in, num_hidden, num_out, learning_rate, activation = 'sigmoid'):\n",
    "        self.lr = learning_rate\n",
    "        #num output nodes\n",
    "        self.num_out = num_out\n",
    "        #list that maintains layers\n",
    "        self.layers = []\n",
    "        np.random.seed(40)\n",
    "        #randomly generate starting weights\n",
    "        hidden = [{\"weights\": [np.random.random()-0.5 for j in range(num_in + 1)], \"activation\": activation} for i in range(num_hidden)]\n",
    "        out = [{\"weights\": [np.random.random()-0.5 for j in range(num_hidden + 1)], \"activation\": \"sigmoid\"} for i in range(num_out)]\n",
    "        self.layers.append(hidden)\n",
    "        self.layers.append(out)\n",
    "    \n",
    "    #Relu activation, accepts inputs and applies relu function to each\n",
    "    def relu_activation(self, inp):\n",
    "        return max(0.0, inp)\n",
    "    \n",
    "    def weighted_sum(self, weights, inputs):\n",
    "        lin_sum = weights[-1] #bias\n",
    "        for i in range(len(inputs)):\n",
    "            lin_sum += inputs[i]*weights[i]\n",
    "        return lin_sum\n",
    "    #Sigmoid activation + transfer. was running into issues with exp overflow when I didn't combine\n",
    "    #these steps into a single method and attempted to separate into sigmoid activation followed by multiplying\n",
    "    #by weights\n",
    "    def sigmoid_activation(self, inp):\n",
    "        return 1.0/(1.0 + np.exp(-inp))\n",
    "    \n",
    "    #applies derivative of sigmoid function to some values\n",
    "    def derivative_sigmoid(self, sig):\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    #applies derivative of relu function to some values\n",
    "    def derivative_relu(self, inp):\n",
    "        return 1.0 if inp > 0 else 0.0\n",
    "    \n",
    "    #forward propogate to make predictions by updating the 'out' field of each neuron\n",
    "    def forward(self, input_vector):\n",
    "        for layer in self.layers:\n",
    "            out = []\n",
    "            for neuron in layer:\n",
    "                w_sum = self.weighted_sum(neuron['weights'], input_vector)\n",
    "                #apply sigmoid activation and weights to make neuron's prediction\n",
    "                if neuron['activation'] == 'sigmoid':\n",
    "                    neuron['out'] = self.sigmoid_activation(w_sum)\n",
    "                #apply relu activation and weights to make prediction\n",
    "                else:\n",
    "                    neuron['out'] = self.relu_activation(w_sum)\n",
    "                out.append(neuron['out'])\n",
    "            #represents the input into the next layer, which is a list of all the current layer's neurons' outputs\n",
    "            input_vector = out\n",
    "        #return the predictions for the last layer\n",
    "        return input_vector\n",
    "    \n",
    "    #back propogate to train the NN on the data\n",
    "    def back_propogate(self, expected):\n",
    "        #move backwards, starting from the last layer to the first\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            #the errors on each neuron\n",
    "            errors = []\n",
    "            if i != len(self.layers) - 1: #bias term\n",
    "                for j in range(len(layer)):\n",
    "                    error_sum = 0\n",
    "                    for neuron in self.layers[i+1]:\n",
    "                        error_sum += neuron['weights'][j] * neuron['delta']\n",
    "                    errors.append(error_sum)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    errors.append(expected[j] - layer[j]['out'])\n",
    "            #use the error times the derivative of the activation function to move towards the weights that minimmize\n",
    "            #the error\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                if neuron['activation'] == 'sigmoid':\n",
    "                    neuron['delta'] = errors[j] * self.derivative_sigmoid(neuron['out'])\n",
    "                else:\n",
    "                    neuron['delta'] = errors[j] * self.derivative_relu(neuron['out'])\n",
    "    \n",
    "    #update the weights of each node\n",
    "    def update_w(self, row):\n",
    "        for i in range(len(self.layers)):\n",
    "            layer_input = row[:-1]\n",
    "            #no error on the first layer (input layer)\n",
    "            if i != 0:\n",
    "                layer_input = [neuron['out'] for neuron in self.layers[i-1]]\n",
    "            #use learning rate times the previously calculated delta value times the input into the layer\n",
    "            #to step towards zero-loss weights in the delta direction with step size tuned by learning rate\n",
    "            for neuron in self.layers[i]:\n",
    "                for j in range(len(layer_input)):\n",
    "                    neuron['weights'][j] += self.lr * neuron['delta'] * layer_input[j]\n",
    "            #bias term\n",
    "            neuron['weights'][-1] += self.lr * neuron['delta']\n",
    "    \n",
    "    #train on data\n",
    "    def train(self, train_data, test_data, max_epochs = 1000):\n",
    "        self.graph(max_epochs)\n",
    "        #update over epochs\n",
    "        for epoch in range(max_epochs):\n",
    "            train_epoch_error = 0\n",
    "            test_epoch_error = 0\n",
    "            \n",
    "            #training data\n",
    "            for row in train_data:\n",
    "                #make predictions using current model\n",
    "                epoch_out = self.forward(row)\n",
    "                \n",
    "                #get error of those predictions\n",
    "                expected_out_prob = [0 for i in range(self.num_out)]\n",
    "                expected_out_prob[int(row[-1])] = 1\n",
    "                train_epoch_error += sum([(expected_out_prob[i] - epoch_out[i])**2 for i in range(len(expected_out_prob))])\n",
    "                \n",
    "                #update the weights based on those calculated errors\n",
    "                self.back_propogate(expected_out_prob)\n",
    "                self.update_w(row)   \n",
    "            \n",
    "            #plot error of this epoch\n",
    "            plt.plot(epoch, train_epoch_error/len(train_data), 'ro', label = \"training\")\n",
    "            \n",
    "            #test data\n",
    "            for row in test_data:\n",
    "                #make prediction based on current model\n",
    "                predicted = self.predict(row)\n",
    "                test_epoch_error += (predicted - row[-1]) ** 2\n",
    "                \n",
    "            #plot performance on test data\n",
    "            plt.plot(epoch, test_epoch_error/len(test_data), 'bo', label = \"test\")\n",
    "    \n",
    "    #predict label of given row of data\n",
    "    def predict(self, row):\n",
    "        out_probs = self.forward(row)\n",
    "        max_prob = float('-inf')\n",
    "        max_ind = -1\n",
    "        for i in range(len(out_probs)):\n",
    "            if out_probs[i] > max_prob:\n",
    "                max_ind = i\n",
    "                max_prob = out_probs[i]\n",
    "        return max_ind\n",
    "    \n",
    "    #set up the graph\n",
    "    def graph(self, max_epochs):\n",
    "        plt.xlim(0, max_epochs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"mean squared error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.genfromtxt('iris.data', delimiter=',', dtype = None, encoding = None)\n",
    "num_label_data = []\n",
    "\n",
    "iris_dict = {\n",
    "    \"Iris-setosa\": 0,\n",
    "    \"Iris-versicolor\": 1,\n",
    "    \"Iris-virginica\": 2\n",
    "}\n",
    "\n",
    "for row in raw_data:\n",
    "    sep_len, sep_width, pet_len, pet_width, label = row\n",
    "    label = iris_dict[label]\n",
    "    #using the max of each attribute (provided in iris.names on the dataset) to normalize\n",
    "    sep_len /= 7.6\n",
    "    sep_width /= 4.4\n",
    "    pet_len /= 6.9\n",
    "    pet_width /= 2.5\n",
    "    num_label_data.append((sep_len, sep_width, pet_len, pet_width, int(label)))\n",
    "\n",
    "np.random.shuffle(num_label_data)\n",
    "\n",
    "train_test_spl = int(len(raw_data) * 0.85)\n",
    "train = np.array(num_label_data[:train_test_spl])\n",
    "test = np.array(num_label_data[train_test_spl:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = NeuralNetwork(4, 20, 3, 0.2, activation = 'linear')\n",
    "net.train(train, test, max_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
