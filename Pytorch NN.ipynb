{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ins, hidden, outs, nonlinear = 'sigmoid', classification = True):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(in_features = ins, out_features = hidden)\n",
    "        self.output = nn.Linear(in_features = hidden, out_features = outs)\n",
    "        self.classification = classification\n",
    "        if self.classification:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "            self.lossfn = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.lossfn = nn.MSELoss()\n",
    "        if(nonlinear == 'sigmoid'):\n",
    "            self.nonlinear = torch.sigmoid\n",
    "        else:\n",
    "            self.nonlinear = F.relu\n",
    "        print(self)\n",
    "    def forward(self, x):\n",
    "        x = self.nonlinear(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        if self.classification:\n",
    "            return self.softmax(x)\n",
    "        else:\n",
    "            return x\n",
    "    def train(self, x_train, y_train, epochs = 1000):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.1)\n",
    "        losses=[]\n",
    "        for i in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.forward(x_train)\n",
    "            loss = self.lossfn(y_hat, y_train)\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch: {i} Loss: {loss}')\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        plt.plot(np.arange(0, epochs),  losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.genfromtxt('iris.data', delimiter=',', dtype = None, encoding = None)\n",
    "num_label_data = []\n",
    "\n",
    "iris_dict = {\n",
    "    \"Iris-setosa\": 0,\n",
    "    \"Iris-versicolor\": 1,\n",
    "    \"Iris-virginica\": 2\n",
    "}\n",
    "\n",
    "for row in raw_data:\n",
    "    sep_len, sep_width, pet_len, pet_width, label = row\n",
    "    label = iris_dict[label]\n",
    "    #using the max of each attribute (provided in iris.names on the dataset) to normalize\n",
    "    sep_len /= 7.6\n",
    "    sep_width /= 4.4\n",
    "    pet_len /= 6.9\n",
    "    pet_width /= 2.5\n",
    "    num_label_data.append((sep_len, sep_width, pet_len, pet_width, int(label)))\n",
    "\n",
    "np.random.shuffle(num_label_data)\n",
    "\n",
    "train_test_spl = int(len(raw_data) * 0.85)\n",
    "train = np.array(num_label_data[:train_test_spl])\n",
    "test = np.array(num_label_data[train_test_spl:])\n",
    "x_train = torch.FloatTensor(train[:, :-1])\n",
    "y_train = torch.LongTensor(train[:, -1])\n",
    "x_test = torch.FloatTensor(test[:, :-1])\n",
    "y_test = torch.FloatTensor(test[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=4, out_features=20, bias=True)\n",
      "  (output): Linear(in_features=20, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (lossfn): CrossEntropyLoss()\n",
      ")\n",
      "Epoch: 0 Loss: 1.0984524488449097\n",
      "Epoch: 10 Loss: 0.8393646478652954\n",
      "Epoch: 20 Loss: 0.6709660291671753\n",
      "Epoch: 30 Loss: 0.6096307635307312\n",
      "Epoch: 40 Loss: 0.5934005379676819\n",
      "Epoch: 50 Loss: 0.5884225368499756\n",
      "Epoch: 60 Loss: 0.5857254862785339\n",
      "Epoch: 70 Loss: 0.5840460658073425\n",
      "Epoch: 80 Loss: 0.5827517509460449\n",
      "Epoch: 90 Loss: 0.581663966178894\n",
      "Epoch: 100 Loss: 0.5807029604911804\n",
      "Epoch: 110 Loss: 0.5798431038856506\n",
      "Epoch: 120 Loss: 0.5790727138519287\n",
      "Epoch: 130 Loss: 0.5783846974372864\n",
      "Epoch: 140 Loss: 0.5777721405029297\n",
      "Epoch: 150 Loss: 0.577226459980011\n",
      "Epoch: 160 Loss: 0.5767408013343811\n",
      "Epoch: 170 Loss: 0.5763074159622192\n",
      "Epoch: 180 Loss: 0.5759192705154419\n",
      "Epoch: 190 Loss: 0.5755696892738342\n",
      "Epoch: 200 Loss: 0.575252890586853\n",
      "Epoch: 210 Loss: 0.5749645829200745\n",
      "Epoch: 220 Loss: 0.5746997594833374\n",
      "Epoch: 230 Loss: 0.5744558572769165\n",
      "Epoch: 240 Loss: 0.5742293000221252\n",
      "Epoch: 250 Loss: 0.5740177631378174\n",
      "Epoch: 260 Loss: 0.5738193988800049\n",
      "Epoch: 270 Loss: 0.573632001876831\n",
      "Epoch: 280 Loss: 0.5734542608261108\n",
      "Epoch: 290 Loss: 0.5732848644256592\n",
      "Epoch: 300 Loss: 0.5731223821640015\n",
      "Epoch: 310 Loss: 0.5729660987854004\n",
      "Epoch: 320 Loss: 0.5728147625923157\n",
      "Epoch: 330 Loss: 0.5726672410964966\n",
      "Epoch: 340 Loss: 0.5725232362747192\n",
      "Epoch: 350 Loss: 0.572381317615509\n",
      "Epoch: 360 Loss: 0.5722413659095764\n",
      "Epoch: 370 Loss: 0.5721023082733154\n",
      "Epoch: 380 Loss: 0.5719636678695679\n",
      "Epoch: 390 Loss: 0.5718251466751099\n",
      "Epoch: 400 Loss: 0.5716861486434937\n",
      "Epoch: 410 Loss: 0.5715467929840088\n",
      "Epoch: 420 Loss: 0.5714069604873657\n",
      "Epoch: 430 Loss: 0.5712671875953674\n",
      "Epoch: 440 Loss: 0.5711285471916199\n",
      "Epoch: 450 Loss: 0.5720607042312622\n",
      "Epoch: 460 Loss: 0.5710771679878235\n",
      "Epoch: 470 Loss: 0.5707563161849976\n",
      "Epoch: 480 Loss: 0.5706773996353149\n",
      "Epoch: 490 Loss: 0.570531964302063\n",
      "Epoch: 500 Loss: 0.5704072713851929\n",
      "Epoch: 510 Loss: 0.5702956914901733\n",
      "Epoch: 520 Loss: 0.5701894164085388\n",
      "Epoch: 530 Loss: 0.5700864195823669\n",
      "Epoch: 540 Loss: 0.5699868202209473\n",
      "Epoch: 550 Loss: 0.5698904395103455\n",
      "Epoch: 560 Loss: 0.5697973370552063\n",
      "Epoch: 570 Loss: 0.5697076320648193\n",
      "Epoch: 580 Loss: 0.5696211457252502\n",
      "Epoch: 590 Loss: 0.569538414478302\n",
      "Epoch: 600 Loss: 0.5694587230682373\n",
      "Epoch: 610 Loss: 0.5693826675415039\n",
      "Epoch: 620 Loss: 0.5693097710609436\n",
      "Epoch: 630 Loss: 0.5692402720451355\n",
      "Epoch: 640 Loss: 0.5691741108894348\n",
      "Epoch: 650 Loss: 0.5691109299659729\n",
      "Epoch: 660 Loss: 0.5690501928329468\n",
      "Epoch: 670 Loss: 0.5689927339553833\n",
      "Epoch: 680 Loss: 0.5689376592636108\n",
      "Epoch: 690 Loss: 0.568885087966919\n",
      "Epoch: 700 Loss: 0.5688347220420837\n",
      "Epoch: 710 Loss: 0.5687864422798157\n",
      "Epoch: 720 Loss: 0.5687404870986938\n",
      "Epoch: 730 Loss: 0.5686963796615601\n",
      "Epoch: 740 Loss: 0.5686541199684143\n",
      "Epoch: 750 Loss: 0.5686134099960327\n",
      "Epoch: 760 Loss: 0.5685745477676392\n",
      "Epoch: 770 Loss: 0.5685369968414307\n",
      "Epoch: 780 Loss: 0.5685011148452759\n",
      "Epoch: 790 Loss: 0.5684664845466614\n",
      "Epoch: 800 Loss: 0.5684332251548767\n",
      "Epoch: 810 Loss: 0.5684010982513428\n",
      "Epoch: 820 Loss: 0.5683700442314148\n",
      "Epoch: 830 Loss: 0.5683402419090271\n",
      "Epoch: 840 Loss: 0.5683113932609558\n",
      "Epoch: 850 Loss: 0.5682836771011353\n",
      "Epoch: 860 Loss: 0.5682567358016968\n",
      "Epoch: 870 Loss: 0.5682305693626404\n",
      "Epoch: 880 Loss: 0.5682054758071899\n",
      "Epoch: 890 Loss: 0.5681809782981873\n",
      "Epoch: 900 Loss: 0.5681574940681458\n",
      "Epoch: 910 Loss: 0.5681345462799072\n",
      "Epoch: 920 Loss: 0.5681123733520508\n",
      "Epoch: 930 Loss: 0.5680907368659973\n",
      "Epoch: 940 Loss: 0.5680697560310364\n",
      "Epoch: 950 Loss: 0.5680494904518127\n",
      "Epoch: 960 Loss: 0.5680297613143921\n",
      "Epoch: 970 Loss: 0.5680105090141296\n",
      "Epoch: 980 Loss: 0.5679919719696045\n",
      "Epoch: 990 Loss: 0.5679738521575928\n"
     ]
    }
   ],
   "source": [
    "net = Net(4, 20, 3)\n",
    "net.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
